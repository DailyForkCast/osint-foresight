# Writing Quality Standards - Pattern Analysis from Netherlands v1 Review
**Date:** 2025-11-07
**Source:** User feedback patterns from Netherlands Strategic Assessment v1 review
**Purpose:** Avoid recurring issues in future reports

---

## RECURRING PATTERNS IN USER FEEDBACK

### 1. **RISK STRATIFICATION - Don't Present Numbers Without Context**

**Bad:**
- "671 unique Chinese institutions identified"
- "361 EU-funded projects with Chinese participation"

**Why it's bad:**
- Implies all Chinese entities are concerning
- No distinction between routine academic collaboration and actual security risks
- "Being Chinese and working internationally is not a crime or automatically evidence of wrong doing"

**Good:**
- Break down by risk level: HIGH/MEDIUM/LOW/UNKNOWN
- Specify which subset warrants security attention
- Lead with risk-stratified numbers, not raw totals
- Acknowledge that majority may be routine partnerships

**Examples from feedback:**
- Item 2: 671 institutions needs risk stratification
- Item 30: "What is the significance of this? How many are known entities of concern?"

---

### 2. **TEMPORAL CONTEXT - Always Include When/Trends**

**Bad:**
- "361 projects" (when? all at once? spread over time?)
- "100% Chinese participation" (in what period?)
- Static snapshots without temporal analysis

**Why it's bad:**
- Can't assess trends (increasing/decreasing?)
- Can't evaluate policy effectiveness
- Missing lag effects and cause-effect relationships
- No context for urgency or evolution

**Good:**
- Year-by-year breakdown
- Trend analysis (growing, stable, declining)
- Correlation with policy changes
- Lag effects acknowledged (e.g., COVID impact timeline)

**Examples from feedback:**
- Item 1: "were these in 2010? or last year? what were the projects on?"
- Item 10: Track partnership trends to assess policy effectiveness
- Item 20: COVID impact timeline and lag effects

---

### 3. **GRANULARITY - Break Down Aggregates**

**Bad:**
- "Europe: 19%" (which countries?)
- "Artificial intelligence" (which subfields?)
- Regional/technology aggregates

**Why it's bad:**
- Masks individual contributions
- Too broad for actionable intelligence
- Loses specificity needed for policy decisions

**Good:**
- Country-level breakdown (Netherlands 15%, Germany 3%, France 1%)
- Technology subfield breakdown (AI → CV, NLP, ML theory, robotics)
- Institutional breakdown (who is doing what specifically)
- Geographic distribution within countries

**Examples from feedback:**
- Item 11: "I want to get rid of 'Europe' and put in the individual countries involved"
- Item 28: "artificial intelligence, for example, is a massive field, we need more granularity"

---

### 4. **DEFINE VAGUE TERMS - Provide Metrics**

**Bad:**
- "Leading institutions"
- "Research excellence"
- "US pressure"
- Subjective characterizations without measurement

**Why it's bad:**
- Unclear what makes something "leading"
- No criteria for evaluation
- Can't verify or replicate analysis
- Vague language undermines credibility

**Good:**
- Define "leading" with specific metrics
- Provide measurement methodology
- Quantify claims with data
- Explain ranking/categorization system

**Metrics to use:**
- Publications count & citations
- Funding levels
- Infrastructure/facilities
- Size (researchers, students)
- Industry partnerships
- Patents

**Examples from feedback:**
- Item 25: "'leading institutions' we need to define what we mean by that"
- Item 24: "What does 'pressure' mean here? What evidence do we have?"

---

### 5. **CITATIONS - Source Everything Legal/Regulatory**

**Bad:**
- "Netherlands government restrictions on EUV exports to China (since 2019)"
- "DUV export licenses required (since 2023)"
- No legal references

**Why it's bad:**
- Can't verify claims
- No traceability to primary sources
- Undermines Zero Fabrication Protocol
- Reader can't find the actual law/regulation

**Good:**
- Exact law/regulation numbers
- Official gazette references
- Government announcement links
- Date enacted, date effective
- Amendment history

**Format:**
- Netherlands Export Control Act [statute number], Article X
- Official Gazette [reference], [date]
- EU Dual-Use Regulation (EU) 2021/821, Annex I
- US Export Administration Regulations 15 CFR §740.2

**Examples from feedback:**
- Item 24: "I want to be able to reference the exact law, regulation, system for each of these"

---

### 6. **AVOID ASSUMPTIONS - Evidence Over Implication**

**Bad:**
- Presenting Chinese collaboration as inherently suspicious
- Assuming all defense-linked universities are malicious
- Overly definitive statements without strong evidence

**Why it's bad:**
- "Just being from China is not a crime or an automatic sign of malevolent intent"
- Creates bias in analysis
- Undermines objectivity
- May miss legitimate collaboration value

**Good:**
- Distinguish between capability and intent
- Acknowledge dual-use research can be legitimate
- Provide evidence for specific concerns
- Caveat strong claims appropriately

**Examples from feedback:**
- Item 2: Risk stratification needed, not blanket concern
- Item 21: "No competitors developing EUV" needs caveat or strong documentation
- Item 30: "Being Chinese and working internationally is not a crime"

---

### 7. **METHODOLOGICAL TRANSPARENCY - Acknowledge Limitations**

**Bad:**
- "Funding success rate" when only funded projects are visible
- Presenting filtered data as comprehensive
- Not disclosing selection bias

**Why it's bad:**
- "Do we have access to every proposal? or just the ones that were chosen?"
- Misleading statistics from incomplete data
- Can't trust analysis if methodology unclear
- Selection bias skews interpretation

**Good:**
- State data sources clearly
- Acknowledge what you DON'T have access to
- Explain filtering/selection criteria
- Disclose limitations prominently
- Don't calculate rates without proper denominators

**Examples from feedback:**
- Item 27b: Funding success rate - can't calculate without total applications
- Item 29: "100% Chinese participation" - filtered dataset makes this meaningless

---

### 8. **MULTI-DIMENSIONAL ANALYSIS - Use Multiple Indicators**

**Bad:**
- Single metric to justify importance
- Publications alone = research excellence
- One data source for complex question

**Why it's bad:**
- "publications should be part of the justification, but let's think more broadly"
- Oversimplifies complex phenomena
- Misses important dimensions
- Incomplete picture for decision-makers

**Good:**
- Multiple metrics for institutional importance:
  - Publications & citations
  - Funding levels & trends
  - Infrastructure & capabilities
  - Size & scale
  - Industry partnerships
  - Patent output
  - International rankings

**Examples from feedback:**
- Item 26: Multi-dimensional institution characterization
- Item 27a: Explain EuroTech significance (not just membership)

---

### 9. **PROGRESSIVE SCOPE - v1 Foundations, v2+ Deep Dives**

**Bad:**
- Trying to answer everything in v1
- OR stopping at surface level permanently

**Why it's bad:**
- Deadline constraints (Nov 23)
- Risk scope creep vs. missing opportunities
- Need framework for future expansion

**Good:**
- v1: Essential findings, framework established
- v1.5: Fill critical gaps identified
- v2: Temporal expansion (2005+), additional sources
- v3-v4: Comprehensive deep dives (800+ suppliers)

**Decision framework:**
- MUST FIX v1: Misleading stats, missing citations, undefined terms
- CAN DEFER: Deep dives needing significant research
- FUTURE RESEARCH: Long-term projects beyond country assessments

**Examples from feedback:**
- Item 7: Keep 10 years for v1, expand to 2005+ for v2
- Item 19: EUV supply chain progressive mapping v1.5-v4
- Item 22: R&D investment deep dive → v2-v3

---

### 10. **EVIDENCE-BASED CLAIMS - No Vague Statements**

**Bad:**
- "US pressure for further restrictions"
- "Leading" without definition
- "Research excellence" without metrics

**Why it's bad:**
- What does "pressure" mean specifically?
- What evidence supports the claim?
- Can't verify or fact-check
- Undermines credibility

**Good:**
- Concrete evidence for claims:
  - Diplomatic communications (if public)
  - Congressional testimony/statements
  - News reports of specific meetings
  - Policy statements from officials
- OR remove/heavily caveat if no solid evidence

**Standards:**
- Every claim traceable to source
- Zero Fabrication Protocol compliance
- If you can't document it, don't state it definitively
- Use "reported," "alleged," "according to [source]" when appropriate

**Examples from feedback:**
- Item 24: "US pressure" - vague, needs evidence
- Item 21: "No competitors" - very definite, needs documentation

---

## SPECIFIC CONTENT STANDARDS

### Technology Analysis
- Always break down broad categories into subfields
- Provide specific areas of excellence, not just domain labels
- Use OpenAlex topics/keywords for granularity
- Map to actual capabilities/applications

### Institutional Profiles
- Multiple dimensions: publications, funding, infrastructure, size
- Quantify with actual data
- Explain significance (why does this matter?)
- Risk stratification where relevant

### Numbers & Statistics
- Always provide context (X out of Y = Z%)
- Get proper denominators (total universe, not just filtered)
- Temporal breakdown (when? trends?)
- Don't present filtered data as comprehensive

### Legal/Regulatory Claims
- Exact statute/regulation numbers
- Official gazette references
- Dates (enacted, effective, amended)
- Link to primary source documents

### Risk Assessment
- HIGH/MEDIUM/LOW/UNKNOWN categorization
- Evidence-based (not assumption-based)
- Distinguish legitimate collaboration from security concerns
- Acknowledge uncertainty where present

---

## WRITING CHECKLIST FOR FUTURE REPORTS

Before finalizing any section, ask:

**Context Questions:**
- [ ] Does every number have temporal context? (when? trends?)
- [ ] Are aggregates broken down to useful granularity?
- [ ] Is every vague term defined with metrics?
- [ ] Do all legal/regulatory claims have exact citations?

**Bias Questions:**
- [ ] Am I presenting data without inappropriate implications?
- [ ] Have I stratified by actual risk vs. identity?
- [ ] Are my assumptions evidence-based?
- [ ] Have I acknowledged legitimate use cases?

**Methodology Questions:**
- [ ] Are data sources clearly stated?
- [ ] Have I disclosed limitations?
- [ ] Do I have proper denominators for rates/percentages?
- [ ] Is selection bias acknowledged?

**Evidence Questions:**
- [ ] Can I trace every claim to a primary source?
- [ ] Are vague statements replaced with concrete evidence?
- [ ] Have I caveated claims appropriately?
- [ ] Does this meet Zero Fabrication Protocol standards?

**Scope Questions:**
- [ ] Is this appropriate for v1 vs. future versions?
- [ ] Have I noted items for future research?
- [ ] Am I trying to answer too much now?
- [ ] Is the framework established for future expansion?

---

## APPLY THESE PATTERNS TO:
- All country strategic assessments (Germany, France, Italy, etc.)
- Technology deep-dive reports
- Policy analysis documents
- Data integration reports
- Any strategic intelligence deliverable

---

**Last Updated:** 2025-11-07
**Review this document before starting any new major writing project**
