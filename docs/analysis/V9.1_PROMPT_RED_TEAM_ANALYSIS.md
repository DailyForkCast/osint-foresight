# Red Team Analysis: v9.1 Master Prompts
**Date:** 2025-09-20
**Purpose:** Critical evaluation of v9.1 prompts for OSINT data discovery and assessment

---

## Executive Summary

The v9.1 prompts represent a **significant improvement** over v8.0, introducing atomic sub-phases, stricter gating, and packet-based data structures. However, several critical areas need attention to maximize data discovery and assessment capabilities.

---

## ðŸŸ¢ STRENGTHS (What v9.1 Does Well)

### 1. Atomic Sub-Phase Architecture
- **Excellent:** Breaking work into B0-B6 sub-phases with gates prevents cascading errors
- **Benefit:** Failures caught early, not propagated through entire analysis
- **Example:** B1 gate stops analysis if sources aren't retrievable with Wayback

### 2. Packet-Based Data Structure (EPKT/NPKT/CPKT)
- **Strong:** Standardized evidence packets enforce consistency
- **Benefit:** Machine-readable handoff between Claude Code and ChatGPT
- **Improvement:** Clear field requirements prevent ambiguity

### 3. Domain-Specific Deduplication Keys
- **Critical Addition:** Patent family_id, LEI for orgs, DOI for papers
- **Benefit:** Prevents double-counting across data sources
- **Example:** Same patent filed in multiple jurisdictions counted once

### 4. Two-Source Independence Definition
- **Clear Rule:** Not independent if same wire copy, same press release, or same primary
- **Benefit:** Prevents echo-chamber corroboration
- **Strength:** Explicit `independence_justification` field required

### 5. Recompute Command Requirements
- **Excellent:** Copy-pastable commands for every number
- **Example:** `curl ... | jq ... | wc -l`
- **Benefit:** Full reproducibility without trusting intermediate steps

---

## ðŸ”´ CRITICAL GAPS (Risks to Data Discovery)

### 1. No Guidance on Finding Data Sources
**Problem:** Prompts assume sources already identified
- Missing: How to discover relevant databases
- Missing: Search strategy formulation
- Missing: Source prioritization logic
- **Impact:** May miss critical data sources entirely

**Recommendation:** Add B0.5 - Source Discovery phase:
```
B0.5 â€” Source Discovery
- Query pattern generation from requirements
- Database inventory check (OpenAlex, TED, USPTO, etc.)
- API availability assessment
- Alternative source identification
```

### 2. Weak on Multi-Language/Non-English Sources
**Problem:** No handling for non-English data (Chinese, Russian, Arabic)
- Missing: Translation quality assessment
- Missing: Transliteration standardization
- Missing: Cultural context preservation
- **Impact:** May miss critical foreign intelligence

**Recommendation:** Add translation packet (TPKT):
```
TPKT: {
  "original_text": "...",
  "original_language": "zh-CN",
  "translation": "...",
  "translation_method": "DeepL|Google|Human",
  "confidence": "high|medium|low",
  "cultural_notes": "..."
}
```

### 3. No Handling of Contradictory Evidence
**Problem:** F7 says "never average" but doesn't provide resolution framework
- Missing: Systematic conflict resolution
- Missing: Source credibility weighting
- Missing: Temporal precedence rules
- **Impact:** Analysis paralysis when sources disagree

**Recommendation:** Add Conflict Resolution Protocol:
```
When sources conflict:
1. Check temporal order (newer usually better)
2. Compare methodologies (direct count > estimate)
3. Evaluate source proximity (primary > secondary)
4. Present both with analysis of likely cause
5. Flag for human review if unresolvable
```

### 4. Coverage Gate Too Rigid (80% minimum)
**Problem:** Many real-world datasets have <80% coverage
- Example: Private company data often 30-40% coverage
- Example: Classified programs have intentional gaps
- **Impact:** Rejects valuable partial intelligence

**Recommendation:** Tiered coverage requirements:
```
Tier A: â‰¥80% coverage OR explicit coverage statement
Tier B: â‰¥50% coverage with caveats
Tier C: Any coverage with clear limitations stated
```

### 5. No Bulk Data Processing Strategy
**Problem:** Prompts optimized for small batches, not 445GB datasets
- Missing: Streaming processing guidance
- Missing: Sampling strategies
- Missing: Incremental processing
- **Impact:** Cannot handle OpenAlex, TED bulk downloads

**Recommendation:** Add bulk processing modes:
```
For datasets >1GB:
- Sample first 1000 records for patterns
- Process in 10K record chunks
- Checkpoint every 100K records
- Aggregate at milestone intervals
```

---

## ðŸŸ¡ OPERATIONAL CONCERNS

### 1. Wayback Requirement May Block Fresh Intelligence
**Issue:** Requiring Wayback URLs prevents using breaking news
- Many sources have 6-month Wayback delay
- Some sources blocked from Wayback
- **Impact:** Miss time-sensitive intelligence

**Mitigation:** Allow alternative preservation:
- PDF download + metadata
- HTML snapshot + headers
- Structured extraction + timestamp

### 2. Packet Overhead for Simple Queries
**Issue:** Full packet structure overkill for basic facts
- Example: "What's the capital of Italy?"
- Requiring EPKT/NPKT adds unnecessary complexity
- **Impact:** Slows simple lookups

**Mitigation:** Add "Quick Facts" mode bypassing packets for Tier C

### 3. No Guidance on API Rate Limits
**Issue:** Recompute commands may hit rate limits
- Missing: Exponential backoff strategy
- Missing: Quota management
- **Impact:** Bulk processing fails

**Add:** Rate limit awareness:
```
If rate_limited:
  - Implement exponential backoff
  - Rotate API keys if available
  - Queue for off-peak processing
```

---

## ðŸ’¡ RECOMMENDATIONS FOR IMPROVEMENT

### 1. Add Source Discovery Phase
```python
def discover_sources(query):
    # Check internal databases first
    internal = check_available_data()

    # Generate search variants
    variants = generate_query_variants(query)

    # Identify relevant APIs
    apis = identify_relevant_apis(query)

    # Prioritize by likelihood of success
    return prioritize_sources(internal + apis)
```

### 2. Add Confidence Calibration
Current "High/Medium/Low" is subjective. Add numeric ranges:
- High: 70-100% (Multiple independent sources, primary data)
- Medium: 40-70% (Single good source or partial corroboration)
- Low: 0-40% (Weak source, significant gaps)

### 3. Add Incremental Processing
For large datasets:
```python
def process_incremental(dataset, checkpoint_every=10000):
    for batch in chunk(dataset, checkpoint_every):
        results = process_batch(batch)
        save_checkpoint(results)
        if results.has_sufficient_evidence():
            return early_exit(results)
```

### 4. Add Alternative Data Paths
When primary sources blocked:
1. Check secondary aggregators
2. Use academic repositories
3. Query government alternatives
4. Check non-English sources
5. Consider FOIA/public records requests

### 5. Add Quality vs Speed Toggle
```
MODE = "thorough" | "rapid" | "exploratory"

thorough: All sub-phases, full validation
rapid: Skip non-critical gates, sample large datasets
exploratory: Find what's available, don't verify everything
```

---

## âœ… VERDICT: Ready with Caveats

### Strengths for Data Assessment:
- âœ… Excellent evidence standards
- âœ… Strong anti-fabrication gates
- âœ… Clear reproducibility requirements
- âœ… Good deduplication logic

### Weaknesses for Data Discovery:
- âš ï¸ No source discovery guidance
- âš ï¸ Poor bulk data handling
- âš ï¸ Rigid coverage requirements
- âš ï¸ No multi-language strategy

### Overall Assessment:
**Score: 7.5/10** - Strong for structured analysis, needs enhancement for discovery

### Critical Additions Needed:
1. **Source discovery protocol** - How to find relevant data
2. **Bulk processing strategy** - Handle 445GB datasets
3. **Conflict resolution framework** - Handle contradictions
4. **Multi-language support** - Non-English intelligence
5. **Flexible coverage thresholds** - Partial data is valuable

---

## ðŸŽ¯ RECOMMENDED IMMEDIATE PATCHES

### Patch 1: Add to B0 (Intake & Plan)
```
B0.1 - Source Discovery
- List available internal datasets
- Generate search patterns
- Check API availability
- Identify alternative sources
```

### Patch 2: Add to F6 (Recompute Fallbacks)
```
If bulk data (>1GB):
- Provide sampling strategy
- Include chunk processing commands
- Add checkpoint instructions
```

### Patch 3: Add to Coverage Requirements
```
Coverage exceptions:
- Private companies: 30% acceptable with disclaimer
- Classified programs: Any coverage valuable
- Foreign entities: Partial better than none
```

### Patch 4: Add Translation Handling
```
For non-English sources:
- Include original text
- Note translation method
- Flag cultural context issues
- Preserve original entity names
```

---

## FINAL RECOMMENDATION

**Adopt v9.1 with the following critical additions:**

1. **Prepend source discovery phase** before B0
2. **Add bulk processing protocols** for large datasets
3. **Implement flexible coverage thresholds** based on data type
4. **Include translation/multilingual handling**
5. **Add conflict resolution framework**

These additions will transform v9.1 from a good **assessment** framework into an excellent **discovery AND assessment** system.

**Risk if unchanged:** May miss 40-60% of available intelligence due to discovery gaps

**Benefit if enhanced:** Complete intelligence cycle from discovery through assessment

---

*Red Team Analysis Complete - Recommend adopting v9.1 with critical patches for discovery capabilities*
