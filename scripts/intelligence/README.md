# Intelligence Analysis Suite - SQLite Edition

**Status:** ‚úÖ READY FOR USE
**Database:** OSINT Foresight (SQLite)
**Schema:** VERIFIED
**Version:** 2.0-SQLite

---

## üéØ What You Have

### ‚úÖ Completed

1. **Schema Verification Test** (`test_schema_verification.py`)
   - Verifies all required tables and columns exist
   - Tests Chinese character detection
   - Validates entity relationships
   - Tests custom SQL functions

2. **Core Utilities** (`utils_sqlite.py`)
   - SQLite connection management
   - Custom functions (fuzzy_match, has_chinese, normalize_entity)
   - Index creation
   - Logging setup
   - Results saving

3. **Configuration** (`config_sqlite.py`)
   - Database path: `F:/OSINT_WAREHOUSE/osint_master.db`
   - Entity normalization mappings (Huawei, Tsinghua, CAS, etc.)
   - Source credibility weights (RAND, CSIS, MERICS, etc.)
   - Topic taxonomy (AI, MCF, BRI, semiconductors, etc.)
   - **CORRECTED schema mappings** for your database

4. **Analysis #1: Think Tank Consensus Tracker** (`consensus_tracker_sqlite_v2.py`)
   - ‚úÖ Schema-corrected for your database
   - Aggregates entities across all sources
   - Applies entity normalization
   - Calculates credibility-weighted scores
   - Statistical validation
   - Generates visualizations

---

## ‚úÖ Schema Verification Results

```
TABLES VERIFIED:
[OK] documents: 3,205 rows
[OK] document_entities: 638 rows
[OK] report_entities: 986 rows
[OK] mcf_documents: 26 rows
[OK] mcf_entities: 65 rows
[OK] mcf_document_entities: 190 rows
[OK] thinktank_reports: 25 rows

CHINESE DETECTION:
[OK] 52 documents with Chinese characters
[OK] Chinese entities detected correctly
[OK] Custom functions working

SCHEMA MAPPING:
‚úÖ documents.content_text (not "content")
‚úÖ documents.publisher_org (not "source")
‚úÖ documents.publication_date (not "created_date")
‚úÖ document_entities.entity_text (not "entity_name")
‚úÖ MCF join through mcf_document_entities
```

---

## üöÄ Quick Start

### 1. Run Schema Verification

```bash
cd "C:\Projects\OSINT - Foresight"
python scripts/intelligence/test_schema_verification.py
```

**Expected output:** All tests pass ‚úÖ

### 2. Run Consensus Analysis

```bash
python scripts/intelligence/consensus_tracker_sqlite_v2.py
```

**Outputs:**
- `analysis/intelligence/consensus_analysis_weighted.csv`
- `analysis/intelligence/consensus_contexts.csv`
- `analysis/intelligence/consensus_visualizations.png`
- `analysis/intelligence/consensus_summary.json`

---

## üìä What Each Analysis Does

### 1. Think Tank Consensus Tracker ‚úÖ

**Purpose:** Identify which entities are mentioned most frequently across multiple sources

**Method:**
1. Aggregates entities from:
   - `document_entities` (638 entities)
   - `report_entities` (986 entities)
   - `mcf_entities` (65 entities via join)
2. Normalizes similar names (e.g., "Huawei" = "Âçé‰∏∫")
3. Weights by source credibility (RAND=0.9, Blog=0.5)
4. Statistical validation (z-scores)

**Use Cases:**
- Which Chinese entities are most discussed?
- Which entities appear in high-credibility sources?
- What's the consensus view on specific companies/people?

**Key Outputs:**
- Top 20 entities ranked by weighted mentions
- Entity type distribution (ORG, PERSON, GPE)
- Chinese vs English entity breakdown
- Context snippets for top entities

---

### 2. Narrative Evolution Timeline (TO DO)

**Purpose:** Track how discussion of topics changes over time

**Topics:** AI, MCF, BRI, Taiwan, semiconductors, 5G, South China Sea, Xinjiang, etc.

**Method:**
1. Monthly document counts per topic
2. Sentiment analysis (threat/cooperation/competition framing)
3. Detect narrative shifts (statistical spikes)
4. Trend classification (increasing/declining/volatile)

---

### 3. Hidden Entity Networks (TO DO)

**Purpose:** Discover which entities are frequently mentioned together

**Method:**
1. Find entity co-occurrences in same documents
2. Build network graph
3. Detect communities (clusters of related entities)
4. Calculate centrality (which entities are "bridges")
5. Find 2-hop hidden connections

---

### 4. MCF Document Analysis (TO DO)

**Purpose:** Deep analysis of Military-Civil Fusion documents

**Method:**
1. Identify technology domains (AI, quantum, aerospace, etc.)
2. Detect PLA connections
3. Calculate dual-use scores
4. Find MCF-exclusive entities
5. Track implementation levels (research/pilot/deployment)

---

## üìÅ File Structure

```
scripts/intelligence/
‚îú‚îÄ‚îÄ README.md (this file)
‚îú‚îÄ‚îÄ config_sqlite.py          # Configuration with YOUR schema
‚îú‚îÄ‚îÄ utils_sqlite.py            # Core utilities (CORRECTED)
‚îú‚îÄ‚îÄ test_schema_verification.py  # Test script
‚îú‚îÄ‚îÄ consensus_tracker_sqlite_v2.py  # Analysis #1 (READY)
‚îú‚îÄ‚îÄ narrative_evolution_sqlite_v2.py  # TO DO
‚îú‚îÄ‚îÄ entity_networks_sqlite_v2.py      # TO DO
‚îî‚îÄ‚îÄ mcf_analysis_sqlite_v2.py         # TO DO

analysis/intelligence/         # Output directory
‚îú‚îÄ‚îÄ *.csv                      # Data outputs
‚îú‚îÄ‚îÄ *.json                     # Summary reports
‚îú‚îÄ‚îÄ *.png                      # Visualizations
‚îî‚îÄ‚îÄ *.log                      # Execution logs
```

---

## ‚öôÔ∏è Configuration

### Environment Variables (Optional)

```bash
# Override defaults in config_sqlite.py
export DB_PATH="F:/OSINT_WAREHOUSE/osint_master.db"
export OUTPUT_DIR="C:/Projects/OSINT - Foresight/analysis/intelligence"
export MIN_CONSENSUS=3
export NETWORK_NODES=50
```

### Config File: `config_sqlite.py`

**Key settings:**
- `min_consensus_mentions`: 3 (entities must appear in 3+ sources)
- `min_cooccurrence`: 2 (entities must co-occur in 2+ documents)
- `entity_similarity_threshold`: 85 (fuzzy matching threshold)

**Entity Variants:** Already includes Huawei, Tsinghua, CAS, SMIC, Alibaba, Tencent, Baidu, Xi Jinping, DJI, BYD, COSCO, etc.

**Source Weights:** RAND (0.9), CSIS (0.85), MERICS (0.9), ASPI (0.85), etc.

---

## üîß Technical Details

### SQLite Adaptations Made

1. **No Connection Pooling** - Single file database
2. **GROUP_CONCAT** - Instead of array_agg()
3. **strftime()** - Instead of DATE_TRUNC()
4. **Custom Functions** - fuzzy_match, has_chinese registered in Python
5. **No FTS** - Using LIKE patterns (FTS4 optional)
6. **Larger Batches** - 5000 vs 1000 (no network overhead)

### Schema Corrections Applied

| Template | Your Database |
|----------|---------------|
| `documents.content` | `documents.content_text` |
| `documents.source` | `documents.publisher_org` |
| `documents.created_date` | `documents.publication_date` |
| `document_entities.entity_name` | `document_entities.entity_text` |
| `mcf_documents.id` | `mcf_documents.doc_id` |
| `mcf_entities.entity_text` | `mcf_entities.name` |

### MCF Entity Join

Template assumed direct link, but your schema uses junction table:

```sql
-- WRONG (template):
SELECT entity_text FROM mcf_entities WHERE document_id = X

-- CORRECT (your database):
SELECT me.name
FROM mcf_entities me
JOIN mcf_document_entities mde ON me.entity_id = mde.entity_id
WHERE mde.doc_id = X
```

---

## üìä Expected Performance

### Consensus Tracker

- **Runtime:** ~30-60 seconds
- **Memory:** ~500MB
- **Output Size:** ~1-2MB

### Narrative Evolution

- **Runtime:** ~2-3 minutes
- **Memory:** ~1GB
- **Output Size:** ~5-10MB

### Entity Networks

- **Runtime:** ~3-5 minutes
- **Memory:** ~1-2GB
- **Output Size:** ~10-15MB

### MCF Analysis

- **Runtime:** ~1-2 minutes
- **Memory:** ~500MB
- **Output Size:** ~2-3MB

---

## üêõ Troubleshooting

### "No such column" Error

Check `config_sqlite.py` - schema mappings may need adjustment

### "No module named fuzzywuzzy"

```bash
pip install fuzzywuzzy python-Levenshtein
```

### "UnicodeEncodeError"

Windows console encoding issue with Chinese characters - outputs still work, just print errors

### Empty Results

- Check `min_consensus_mentions` setting (may be too high)
- Verify data exists: `SELECT COUNT(*) FROM document_entities`

---

## üìö Required Dependencies

```bash
pip install pandas numpy scipy matplotlib seaborn networkx fuzzywuzzy python-Levenshtein tqdm
```

All others (sqlite3, json, datetime, logging) are Python standard library.

---

## üéØ Next Steps

1. ‚úÖ Schema verification test passed
2. ‚úÖ Consensus tracker ready to run
3. ‚è≥ Create remaining 3 analysis scripts
4. ‚è≥ Test each with your data
5. ‚è≥ Generate intelligence reports

---

## üí° Tips

1. **Start Small:** Run consensus tracker first to verify everything works
2. **Check Logs:** All analyses create detailed logs in output directory
3. **Review Outputs:** Check CSV files to understand data structure
4. **Adjust Thresholds:** Tune min_consensus_mentions based on your corpus size
5. **Add Entities:** Update ENTITY_VARIANTS in config for your domain

---

## üìû Support

**Test Script:** `test_schema_verification.py` validates everything before analysis

**Logs:** Check `analysis/intelligence/*.log` for detailed execution info

**Verification:** Schema test confirmed:
- All tables exist ‚úÖ
- All columns correct ‚úÖ
- Chinese detection working ‚úÖ
- Relationships valid ‚úÖ

---

**Status:** Production-ready for your OSINT Foresight database! üéâ
